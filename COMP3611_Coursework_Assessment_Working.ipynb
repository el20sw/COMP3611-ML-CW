{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d67FXFqGEIHQ"
   },
   "source": [
    "# **Predict Cancer Mortality Rates in US Counties**\n",
    "\n",
    "The provided dataset comprises data collected from multiple counties in the US. The regression task for this assessment is to predict cancer mortality rates in \"unseen\" US counties, given some training data. The training data ('Training_data.csv') comprises various features/predictors related to socio-economic characteristics, amongst other types of information for specific counties in the country. The corresponding target variables for the training set are provided in a separate CSV file ('Training_data_targets.csv'). Use the notebooks provided for lab sessions throughout this module to provide solutions to the exercises listed below. Throughout all exercises, text describing your code and answering any questions included in the exercise descriptions should be provided as part of your submitted solution. (Total Marks for this Assessment is 40)\n",
    "\n",
    "Note - We also provide an example test data set ('Test_data_example.csv' and 'Test_data_example_targets.csv'). This is just an example of the final test set (which will not be provided to you) that will be used to evaluate your solutions when your submitted solutions are being marked. Part of this assessment requires you to write an inference script that evaluates the regression models you have trained on the final test data set such that we are able to run the inference script ourselves on the test data (you can use the example test data to verify that it works prior to submission).\n",
    "\n",
    "The list of predictors/features available in this data set are described below:\n",
    "\n",
    "**Data Dictionary**\n",
    "\n",
    "avgAnnCount: Mean number of reported cases of cancer diagnosed annually\n",
    "\n",
    "avgDeathsPerYear: Mean number of reported mortalities due to cancer\n",
    "\n",
    "incidenceRate: Mean per capita (100,000) cancer diagoses\n",
    "\n",
    "medianIncome: Median income per county \n",
    "\n",
    "popEst2015: Population of county \n",
    "\n",
    "povertyPercent: Percent of populace in poverty \n",
    "\n",
    "MedianAge: Median age of county residents \n",
    "\n",
    "MedianAgeMale: Median age of male county residents \n",
    "\n",
    "MedianAgeFemale: Median age of female county residents \n",
    "\n",
    "AvgHouseholdSize: Mean household size of county \n",
    "\n",
    "PercentMarried: Percent of county residents who are married \n",
    "\n",
    "PctNoHS18_24: Percent of county residents ages 18-24 highest education attained: less than high school \n",
    "\n",
    "PctHS18_24: Percent of county residents ages 18-24 highest education attained: high school diploma \n",
    "\n",
    "PctSomeCol18_24: Percent of county residents ages 18-24 highest education attained: some college \n",
    "\n",
    "PctBachDeg18_24: Percent of county residents ages 18-24 highest education attained: bachelor's degree \n",
    "\n",
    "PctHS25_Over: Percent of county residents ages 25 and over highest education attained: high school diploma \n",
    "\n",
    "PctBachDeg25_Over: Percent of county residents ages 25 and over highest education attained: bachelor's degree \n",
    "\n",
    "PctEmployed16_Over: Percent of county residents ages 16 and over employed \n",
    "\n",
    "PctUnemployed16_Over: Percent of county residents ages 16 and over unemployed \n",
    "\n",
    "PctPrivateCoverage: Percent of county residents with private health coverage \n",
    "\n",
    "PctPrivateCoverageAlone: Percent of county residents with private health coverage alone (no public assistance) \n",
    "\n",
    "PctEmpPrivCoverage: Percent of county residents with employee-provided private health coverage \n",
    "\n",
    "PctPublicCoverage: Percent of county residents with government-provided health coverage \n",
    "\n",
    "PctPubliceCoverageAlone: Percent of county residents with government-provided health coverage alone \n",
    "\n",
    "PctWhite: Percent of county residents who identify as White \n",
    "\n",
    "PctBlack: Percent of county residents who identify as Black \n",
    "\n",
    "PctAsian: Percent of county residents who identify as Asian \n",
    "\n",
    "PctOtherRace: Percent of county residents who identify in a category which is not White, Black, or Asian \n",
    "\n",
    "PctMarriedHouseholds: Percent of married households \n",
    "\n",
    "BirthRate: Number of live births relative to number of women in county \n",
    "\n",
    "studyPerCap: Per capita number of cancer-related clinical trials per county (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kB3aG5f-D4Q4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "## Define paths to the training data and targets files\n",
    "training_data_path = 'Training_data.csv'\n",
    "training_targets_path = 'Training_data_targets.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHfmuohnJcc_"
   },
   "source": [
    "# **Exercise 1**\n",
    "\n",
    "Read in the training data and targets files. The training data comprises features/predictors while the targets file comprises the targets (i.e. cancer mortality rates in US counties) you need to train models to predict. Plot histograms of all features to visualise their distributions and identify outliers. Do you notice any unusual values for any of the features? If so comment on these in the text accompanying your code. Compute correlations of all features with the target variable (across the data set) and sort them according the strength of correlations. Which are the top five features with strongest correlations to the targets? Plot these correlations using the scatter matrix plotting function available in pandas and comment on at least two sets of features that show visible correlations to each other. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OlTEKBhiM0U5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avgAnnCount</th>\n",
       "      <th>avgDeathsPerYear</th>\n",
       "      <th>incidenceRate</th>\n",
       "      <th>medIncome</th>\n",
       "      <th>popEst2015</th>\n",
       "      <th>povertyPercent</th>\n",
       "      <th>studyPerCap</th>\n",
       "      <th>MedianAge</th>\n",
       "      <th>MedianAgeMale</th>\n",
       "      <th>MedianAgeFemale</th>\n",
       "      <th>...</th>\n",
       "      <th>PctEmpPrivCoverage</th>\n",
       "      <th>PctPublicCoverage</th>\n",
       "      <th>PctPublicCoverageAlone</th>\n",
       "      <th>PctWhite</th>\n",
       "      <th>PctBlack</th>\n",
       "      <th>PctAsian</th>\n",
       "      <th>PctOtherRace</th>\n",
       "      <th>PctMarriedHouseholds</th>\n",
       "      <th>BirthRate</th>\n",
       "      <th>TARGET_deathRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>404.300000</td>\n",
       "      <td>33975</td>\n",
       "      <td>8251</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.3</td>\n",
       "      <td>50.8</td>\n",
       "      <td>51.9</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>49.7</td>\n",
       "      <td>20.6</td>\n",
       "      <td>96.684036</td>\n",
       "      <td>0.438181</td>\n",
       "      <td>0.082899</td>\n",
       "      <td>0.272383</td>\n",
       "      <td>51.926207</td>\n",
       "      <td>5.041436</td>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>403.800000</td>\n",
       "      <td>47363</td>\n",
       "      <td>22702</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.8</td>\n",
       "      <td>39.8</td>\n",
       "      <td>42.7</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>31.6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>92.295459</td>\n",
       "      <td>2.102845</td>\n",
       "      <td>0.609648</td>\n",
       "      <td>0.879131</td>\n",
       "      <td>50.949545</td>\n",
       "      <td>6.329661</td>\n",
       "      <td>137.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>77222</td>\n",
       "      <td>9899</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.1</td>\n",
       "      <td>36.9</td>\n",
       "      <td>39.8</td>\n",
       "      <td>...</td>\n",
       "      <td>54.3</td>\n",
       "      <td>18.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>95.690422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523871</td>\n",
       "      <td>0.118612</td>\n",
       "      <td>64.532156</td>\n",
       "      <td>5.148130</td>\n",
       "      <td>126.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>429.600000</td>\n",
       "      <td>80650</td>\n",
       "      <td>48904</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.5</td>\n",
       "      <td>42.7</td>\n",
       "      <td>44.1</td>\n",
       "      <td>...</td>\n",
       "      <td>55.6</td>\n",
       "      <td>28.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>89.606996</td>\n",
       "      <td>7.407407</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>62.344481</td>\n",
       "      <td>5.627462</td>\n",
       "      <td>173.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>407.500000</td>\n",
       "      <td>42839</td>\n",
       "      <td>22255</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.1</td>\n",
       "      <td>30.2</td>\n",
       "      <td>31.6</td>\n",
       "      <td>...</td>\n",
       "      <td>46.5</td>\n",
       "      <td>26.8</td>\n",
       "      <td>18.1</td>\n",
       "      <td>79.587990</td>\n",
       "      <td>2.948701</td>\n",
       "      <td>8.482564</td>\n",
       "      <td>5.637090</td>\n",
       "      <td>63.005948</td>\n",
       "      <td>10.436469</td>\n",
       "      <td>179.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>335.000000</td>\n",
       "      <td>155</td>\n",
       "      <td>445.700000</td>\n",
       "      <td>41608</td>\n",
       "      <td>61109</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.728403</td>\n",
       "      <td>41.2</td>\n",
       "      <td>39.8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.4</td>\n",
       "      <td>42.6</td>\n",
       "      <td>23.4</td>\n",
       "      <td>95.398451</td>\n",
       "      <td>2.154399</td>\n",
       "      <td>0.448024</td>\n",
       "      <td>0.106749</td>\n",
       "      <td>49.736708</td>\n",
       "      <td>5.379260</td>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>113.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>497.300000</td>\n",
       "      <td>61259</td>\n",
       "      <td>17299</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.9</td>\n",
       "      <td>44.9</td>\n",
       "      <td>47.3</td>\n",
       "      <td>...</td>\n",
       "      <td>43.6</td>\n",
       "      <td>34.8</td>\n",
       "      <td>19.9</td>\n",
       "      <td>92.609104</td>\n",
       "      <td>3.320038</td>\n",
       "      <td>0.786016</td>\n",
       "      <td>0.997184</td>\n",
       "      <td>53.404362</td>\n",
       "      <td>3.227666</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>571.000000</td>\n",
       "      <td>210</td>\n",
       "      <td>457.200000</td>\n",
       "      <td>49790</td>\n",
       "      <td>118212</td>\n",
       "      <td>12.6</td>\n",
       "      <td>676.750245</td>\n",
       "      <td>35.4</td>\n",
       "      <td>34.5</td>\n",
       "      <td>36.3</td>\n",
       "      <td>...</td>\n",
       "      <td>56.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>13.7</td>\n",
       "      <td>91.484690</td>\n",
       "      <td>1.389174</td>\n",
       "      <td>4.463981</td>\n",
       "      <td>0.651015</td>\n",
       "      <td>47.707412</td>\n",
       "      <td>4.937288</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>1962.667684</td>\n",
       "      <td>7</td>\n",
       "      <td>453.549422</td>\n",
       "      <td>50886</td>\n",
       "      <td>2640</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.4</td>\n",
       "      <td>45.3</td>\n",
       "      <td>50.1</td>\n",
       "      <td>...</td>\n",
       "      <td>35.1</td>\n",
       "      <td>32.3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>96.892139</td>\n",
       "      <td>0.987203</td>\n",
       "      <td>0.548446</td>\n",
       "      <td>0.146252</td>\n",
       "      <td>62.436975</td>\n",
       "      <td>8.951965</td>\n",
       "      <td>136.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>741.000000</td>\n",
       "      <td>308</td>\n",
       "      <td>478.400000</td>\n",
       "      <td>45556</td>\n",
       "      <td>119980</td>\n",
       "      <td>13.8</td>\n",
       "      <td>300.050008</td>\n",
       "      <td>44.0</td>\n",
       "      <td>43.2</td>\n",
       "      <td>44.6</td>\n",
       "      <td>...</td>\n",
       "      <td>44.8</td>\n",
       "      <td>39.2</td>\n",
       "      <td>18.9</td>\n",
       "      <td>96.006176</td>\n",
       "      <td>0.790178</td>\n",
       "      <td>0.771187</td>\n",
       "      <td>0.406236</td>\n",
       "      <td>47.201757</td>\n",
       "      <td>4.823686</td>\n",
       "      <td>193.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2438 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avgAnnCount  avgDeathsPerYear  incidenceRate  medIncome  popEst2015  \\\n",
       "0       59.000000                30     404.300000      33975        8251   \n",
       "1      114.000000                41     403.800000      47363       22702   \n",
       "2       33.000000                11     352.000000      77222        9899   \n",
       "3      254.000000               100     429.600000      80650       48904   \n",
       "4       75.000000                32     407.500000      42839       22255   \n",
       "...           ...               ...            ...        ...         ...   \n",
       "2433   335.000000               155     445.700000      41608       61109   \n",
       "2434   113.000000                37     497.300000      61259       17299   \n",
       "2435   571.000000               210     457.200000      49790      118212   \n",
       "2436  1962.667684                 7     453.549422      50886        2640   \n",
       "2437   741.000000               308     478.400000      45556      119980   \n",
       "\n",
       "      povertyPercent  studyPerCap  MedianAge  MedianAgeMale  MedianAgeFemale  \\\n",
       "0               20.5     0.000000       51.3           50.8             51.9   \n",
       "1               13.8     0.000000       40.8           39.8             42.7   \n",
       "2                6.8     0.000000       38.1           36.9             39.8   \n",
       "3                7.5     0.000000       43.5           42.7             44.1   \n",
       "4               14.6     0.000000       31.1           30.2             31.6   \n",
       "...              ...          ...        ...            ...              ...   \n",
       "2433            17.5    32.728403       41.2           39.8             42.0   \n",
       "2434             9.0     0.000000       45.9           44.9             47.3   \n",
       "2435            12.6   676.750245       35.4           34.5             36.3   \n",
       "2436            10.4     0.000000       47.4           45.3             50.1   \n",
       "2437            13.8   300.050008       44.0           43.2             44.6   \n",
       "\n",
       "      ...  PctEmpPrivCoverage  PctPublicCoverage  PctPublicCoverageAlone  \\\n",
       "0     ...                26.0               49.7                    20.6   \n",
       "1     ...                46.8               31.6                    13.0   \n",
       "2     ...                54.3               18.2                     8.6   \n",
       "3     ...                55.6               28.8                    13.5   \n",
       "4     ...                46.5               26.8                    18.1   \n",
       "...   ...                 ...                ...                     ...   \n",
       "2433  ...                42.4               42.6                    23.4   \n",
       "2434  ...                43.6               34.8                    19.9   \n",
       "2435  ...                56.5               28.6                    13.7   \n",
       "2436  ...                35.1               32.3                    12.6   \n",
       "2437  ...                44.8               39.2                    18.9   \n",
       "\n",
       "       PctWhite  PctBlack  PctAsian  PctOtherRace  PctMarriedHouseholds  \\\n",
       "0     96.684036  0.438181  0.082899      0.272383             51.926207   \n",
       "1     92.295459  2.102845  0.609648      0.879131             50.949545   \n",
       "2     95.690422  0.000000  0.523871      0.118612             64.532156   \n",
       "3     89.606996  7.407407  0.870370      0.450617             62.344481   \n",
       "4     79.587990  2.948701  8.482564      5.637090             63.005948   \n",
       "...         ...       ...       ...           ...                   ...   \n",
       "2433  95.398451  2.154399  0.448024      0.106749             49.736708   \n",
       "2434  92.609104  3.320038  0.786016      0.997184             53.404362   \n",
       "2435  91.484690  1.389174  4.463981      0.651015             47.707412   \n",
       "2436  96.892139  0.987203  0.548446      0.146252             62.436975   \n",
       "2437  96.006176  0.790178  0.771187      0.406236             47.201757   \n",
       "\n",
       "      BirthRate  TARGET_deathRate  \n",
       "0      5.041436             199.5  \n",
       "1      6.329661             137.1  \n",
       "2      5.148130             126.9  \n",
       "3      5.627462             173.8  \n",
       "4     10.436469             179.8  \n",
       "...         ...               ...  \n",
       "2433   5.379260             201.5  \n",
       "2434   3.227666             160.0  \n",
       "2435   4.937288             160.0  \n",
       "2436   8.951965             136.2  \n",
       "2437   4.823686             193.2  \n",
       "\n",
       "[2438 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the training data and targets files\n",
    "train = pd.read_csv(training_data_path)\n",
    "targets = pd.read_csv(training_targets_path)\n",
    "\n",
    "train_targets = train.join(targets)\n",
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicates\n",
    "train_targets.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgAnnCount                   0\n",
       "avgDeathsPerYear              0\n",
       "incidenceRate                 0\n",
       "medIncome                     0\n",
       "popEst2015                    0\n",
       "povertyPercent                0\n",
       "studyPerCap                   0\n",
       "MedianAge                     0\n",
       "MedianAgeMale                 0\n",
       "MedianAgeFemale               0\n",
       "AvgHouseholdSize              0\n",
       "PercentMarried                0\n",
       "PctNoHS18_24                  0\n",
       "PctHS18_24                    0\n",
       "PctSomeCol18_24            1829\n",
       "PctBachDeg18_24               0\n",
       "PctHS25_Over                  0\n",
       "PctBachDeg25_Over             0\n",
       "PctEmployed16_Over          119\n",
       "PctUnemployed16_Over          0\n",
       "PctPrivateCoverage            0\n",
       "PctPrivateCoverageAlone     483\n",
       "PctEmpPrivCoverage            0\n",
       "PctPublicCoverage             0\n",
       "PctPublicCoverageAlone        0\n",
       "PctWhite                      0\n",
       "PctBlack                      0\n",
       "PctAsian                      0\n",
       "PctOtherRace                  0\n",
       "PctMarriedHouseholds          0\n",
       "BirthRate                     0\n",
       "TARGET_deathRate              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "train_targets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of all features to visualise their distributions and identify outliers\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_targets.hist(bins=100, figsize=(20,15))\n",
    "\n",
    "# median age seems to have an outlier in that in at least one county the median age exceeds 500 years\n",
    "# train[\"MedianAge\"].hist(bins=50)\n",
    "\n",
    "# average household size seems to have near zero or zero values -\n",
    "# if realistic, suggests that most homes in a county are unoccupied which seems unlikely\n",
    "# train[\"AvgHouseholdSize\"].hist(bins=50)\n",
    "\n",
    "# PctSomeCol18_24 has very few data entries, only 609 vs 2438 for most other features\n",
    "# PctEmployed16_Over and PctPrivateCoverageAlone are also missing values\n",
    "\n",
    "# counties have 0% asian, black and other races which is unlikely given the size of the counties\n",
    "\n",
    "# counties birth rate is 0 in some counties which again seems unlikely\n",
    "\n",
    "# avgAnnCount has an extremely large variance with some values as low as 6 and others above 20,000 \n",
    "# and also a spike around 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations of all features with the target variable (across the data set) \n",
    "# and sort them according the strength of correlations\n",
    "\n",
    "corr = train_targets.corr().abs()\n",
    "corr_TARGET = corr[\"TARGET_deathRate\"].sort_values(ascending=False)\n",
    "top5_corr = corr_TARGET.drop(\"TARGET_deathRate\").nlargest(n=5)\n",
    "\n",
    "print(top5_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot these correlations using the scatter matrix plotting function available in pandas and \n",
    "# comment on at least two sets of features that show visible correlations to each other\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attr = top5_corr.index\n",
    "scatter_matrix(train_targets[attr], figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PctPublicCoverageAlone and povertyPercent have an obvious positive correlation\n",
    "# medIncome and PctPublicCoverageAlone also has an obvious negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wSA10GzM_Xu"
   },
   "source": [
    "# **Exercise 2**\n",
    "\n",
    "Create an ML pipeline using scikit-learn (as demonstrated in the lab notebooks) to pre-process the training data. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and targets\n",
    "X = train_targets.drop(['TARGET_deathRate'],axis=1)\n",
    "y = train_targets['TARGET_deathRate'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m sss \u001b[38;5;241m=\u001b[39m StratifiedShuffleSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get train and test sets\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m sss\u001b[38;5;241m.\u001b[39msplit(X,y):\n\u001b[1;32m     18\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mloc[train_index], X\u001b[38;5;241m.\u001b[39mloc[test_index]\n\u001b[1;32m     19\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mloc[train_index], y\u001b[38;5;241m.\u001b[39mloc[test_index]\n",
      "File \u001b[0;32m~/University/COMP3611/Coursework/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:1622\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m \n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1621\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1623\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/University/COMP3611/Coursework/notebook/jupyterenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:1968\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1966\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   1967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1969\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1973\u001b[0m     )\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[1;32m   1976\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1977\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1978\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   1979\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# Feature pre-processing\n",
    "X.drop(['PctSomeCol18_24', 'PctPrivateCoverageAlone'], axis=1,inplace=True)\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply Pipeline and modify targets\n",
    "X = pipeline.fit_transform(X)\n",
    "y = np.ceil()\n",
    "\n",
    "# Use StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "\n",
    "# Get train and test sets\n",
    "for train_index, test_index in sss.split(X,y):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "# from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data set into train and test\n",
    "\n",
    "\"\"\"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(train_targets, train_targets[\"TARGET_deathRate\"]):\n",
    "    strat_train_set = train_targets.loc[train_index]\n",
    "    strat_test_set = train_targets.loc[test_index]\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop((\"TARGET_deathRate\"), axis=1, inplace=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JJ21KooNRVT"
   },
   "outputs": [],
   "source": [
    "# Drop features missing too much data\n",
    "\n",
    "train.drop(['PctSomeCol18_24', 'PctPrivateCoverageAlone'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared = pipeline.fit_transform(train)\n",
    "targets_prepared = pipeline.fit_transform(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiM5kym6NRww"
   },
   "source": [
    "# **Exercise 3**\n",
    "\n",
    "Fit linear regression models to the pre-processed data using: Ordinary least squares (OLS), Lasso and Ridge models. Choose suitable regularisation weights for Lasso and Ridge regression and include a description in text of how they were chosen. In your submitted solution make sure you set the values for the regularisation weights equal to those you identify from your experiment(s). Quantitatively compare your results from all three models and report the best performing one. Include code for all steps above. (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bv9wFbldPo45"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "\n",
    "ols = lm.LinearRegression()\n",
    "ridge = lm.Ridge(alpha=0.5)\n",
    "lasso = lm.Lasso(alpha=0.5)\n",
    "\n",
    "\"\"\" Testing\n",
    "some_train=train.iloc[:5]\n",
    "some_targets=targets.iloc[:5]\n",
    "some_data_prepared=full_pipeline.fit_transform(some_train)\n",
    "\n",
    "print(\"Predictions:\", ols.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_targets))\n",
    "End testing \"\"\"\n",
    "\n",
    "ols.fit(train_prepared, targets)\n",
    "ridge.fit(train_prepared, targets)\n",
    "lasso.fit(train_prepared, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "ols_train_predictions = ols.predict(train_prepared)\n",
    "ols_mse=mean_squared_error(targets, train_predictions)\n",
    "ols_rmse=np.sqrt(ols_mse)\n",
    "print(ols_rmse)\n",
    "\n",
    "ridge_train_prediction = ridge.predict(train_prepared)\n",
    "ridge_mse=mean_squared_error(targets, train_predictions)\n",
    "ridge_rmse=np.sqrt(ridge_mse)\n",
    "print(ridge_rmse)\n",
    "\n",
    "lasso_train_prediction = lasso.predict(train_prepared)\n",
    "lasso_mse=mean_squared_error(targets, train_predictions)\n",
    "lasso_rmse=np.sqrt(lasso_mse)\n",
    "print(lasso_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdx4FmjqPpVJ"
   },
   "source": [
    "# **Exercise 4**\n",
    "\n",
    "Use Lasso regression and the best regularisation weight identified from Exercise 3 to identify the five most important/relevant features for the provided data set and regression task. Report what these are desceding order of their importance. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vC63VPnQXI_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQvZqdqGQ_pH"
   },
   "source": [
    "# **Exercise 5**\n",
    "\n",
    "Fit a Random Forest regression model to the training data and quantitatively evaluate and compare the Random Forest regression model with the best linear regression model identified from Exercise 3. Report which model provides the best results. Next, report the top five most important/relevant features for the provided data set and regression task identified using the Random Forest model. Comment on how these compare with the features identified from Lasso regression? (14 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbFXu6UiQ-gv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg-Tksx5QXiL"
   },
   "source": [
    "# **Exercise 6**\n",
    "\n",
    "Use the provided test example data ('Test_data_example.csv' and 'Test_data_example_targets.csv') to write an inference script to evaluate the best regression model identified from preceding exercises. First re-train the chosen regression model using all of the provided training data and test your predictions on the provided example test data. Note - the final evaluation of your submission will be done by replacing this example test data with held out (unseen) test data that is not provided to you. But the format of this \"unseen\" test data will be identical to the example test data provided to you. Use the code snippet provided below to prepare your inference script to predict targets for the unseen test data. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcmuUJMODD2Q"
   },
   "outputs": [],
   "source": [
    "## Read in the provided example test data\n",
    "test_data_path = 'Test_data_example.csv'\n",
    "test_targets_path ='Test_data_example_targets.csv'\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_targets = pd.read_csv(test_targets_path)\n",
    "## Retrain your chosen regression model here \n",
    "# For example: lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X_train,y_train) where X_train and y_train is provided training data\n",
    "# Next write the lines of code required to predict on unseen test data and evaluate your predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5611M-Coursework Assessment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
