{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d67FXFqGEIHQ"
   },
   "source": [
    "# **Predict Cancer Mortality Rates in US Counties**\n",
    "\n",
    "The provided dataset comprises data collected from multiple counties in the US. The regression task for this assessment is to predict cancer mortality rates in \"unseen\" US counties, given some training data. The training data ('Training_data.csv') comprises various features/predictors related to socio-economic characteristics, amongst other types of information for specific counties in the country. The corresponding target variables for the training set are provided in a separate CSV file ('Training_data_targets.csv'). Use the notebooks provided for lab sessions throughout this module to provide solutions to the exercises listed below. Throughout all exercises, text describing your code and answering any questions included in the exercise descriptions should be provided as part of your submitted solution. (Total Marks for this Assessment is 40)\n",
    "\n",
    "Note - We also provide an example test data set ('Test_data_example.csv' and 'Test_data_example_targets.csv'). This is just an example of the final test set (which will not be provided to you) that will be used to evaluate your solutions when your submitted solutions are being marked. Part of this assessment requires you to write an inference script that evaluates the regression models you have trained on the final test data set such that we are able to run the inference script ourselves on the test data (you can use the example test data to verify that it works prior to submission).\n",
    "\n",
    "The list of predictors/features available in this data set are described below:\n",
    "\n",
    "**Data Dictionary**\n",
    "\n",
    "avgAnnCount: Mean number of reported cases of cancer diagnosed annually\n",
    "\n",
    "avgDeathsPerYear: Mean number of reported mortalities due to cancer\n",
    "\n",
    "incidenceRate: Mean per capita (100,000) cancer diagoses\n",
    "\n",
    "medianIncome: Median income per county \n",
    "\n",
    "popEst2015: Population of county \n",
    "\n",
    "povertyPercent: Percent of populace in poverty \n",
    "\n",
    "MedianAge: Median age of county residents \n",
    "\n",
    "MedianAgeMale: Median age of male county residents \n",
    "\n",
    "MedianAgeFemale: Median age of female county residents \n",
    "\n",
    "AvgHouseholdSize: Mean household size of county \n",
    "\n",
    "PercentMarried: Percent of county residents who are married \n",
    "\n",
    "PctNoHS18_24: Percent of county residents ages 18-24 highest education attained: less than high school \n",
    "\n",
    "PctHS18_24: Percent of county residents ages 18-24 highest education attained: high school diploma \n",
    "\n",
    "PctSomeCol18_24: Percent of county residents ages 18-24 highest education attained: some college \n",
    "\n",
    "PctBachDeg18_24: Percent of county residents ages 18-24 highest education attained: bachelor's degree \n",
    "\n",
    "PctHS25_Over: Percent of county residents ages 25 and over highest education attained: high school diploma \n",
    "\n",
    "PctBachDeg25_Over: Percent of county residents ages 25 and over highest education attained: bachelor's degree \n",
    "\n",
    "PctEmployed16_Over: Percent of county residents ages 16 and over employed \n",
    "\n",
    "PctUnemployed16_Over: Percent of county residents ages 16 and over unemployed \n",
    "\n",
    "PctPrivateCoverage: Percent of county residents with private health coverage \n",
    "\n",
    "PctPrivateCoverageAlone: Percent of county residents with private health coverage alone (no public assistance) \n",
    "\n",
    "PctEmpPrivCoverage: Percent of county residents with employee-provided private health coverage \n",
    "\n",
    "PctPublicCoverage: Percent of county residents with government-provided health coverage \n",
    "\n",
    "PctPubliceCoverageAlone: Percent of county residents with government-provided health coverage alone \n",
    "\n",
    "PctWhite: Percent of county residents who identify as White \n",
    "\n",
    "PctBlack: Percent of county residents who identify as Black \n",
    "\n",
    "PctAsian: Percent of county residents who identify as Asian \n",
    "\n",
    "PctOtherRace: Percent of county residents who identify in a category which is not White, Black, or Asian \n",
    "\n",
    "PctMarriedHouseholds: Percent of married households \n",
    "\n",
    "BirthRate: Number of live births relative to number of women in county\n",
    "\n",
    "studyPerCap: Per capita number of cancer-related clinical trials per county (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kB3aG5f-D4Q4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Define paths to the training data and targets files\n",
    "training_data_path = 'Training_data.csv'\n",
    "training_targets_path = 'Training_data_targets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHfmuohnJcc_"
   },
   "source": [
    "# **Exercise 1**\n",
    "\n",
    "Read in the training data and targets files. The training data comprises features/predictors while the targets file comprises the targets (i.e. cancer mortality rates in US counties) you need to train models to predict. Plot histograms of all features to visualise their distributions and identify outliers. Do you notice any unusual values for any of the features? If so comment on these in the text accompanying your code. Compute correlations of all features with the target variable (across the data set) and sort them according the strength of correlations. Which are the top five features with strongest correlations to the targets? Plot these correlations using the scatter matrix plotting function available in pandas and comment on at least two sets of features that show visible correlations to each other. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OlTEKBhiM0U5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avgAnnCount</th>\n",
       "      <th>avgDeathsPerYear</th>\n",
       "      <th>incidenceRate</th>\n",
       "      <th>medIncome</th>\n",
       "      <th>popEst2015</th>\n",
       "      <th>povertyPercent</th>\n",
       "      <th>studyPerCap</th>\n",
       "      <th>MedianAge</th>\n",
       "      <th>MedianAgeMale</th>\n",
       "      <th>MedianAgeFemale</th>\n",
       "      <th>...</th>\n",
       "      <th>PctEmpPrivCoverage</th>\n",
       "      <th>PctPublicCoverage</th>\n",
       "      <th>PctPublicCoverageAlone</th>\n",
       "      <th>PctWhite</th>\n",
       "      <th>PctBlack</th>\n",
       "      <th>PctAsian</th>\n",
       "      <th>PctOtherRace</th>\n",
       "      <th>PctMarriedHouseholds</th>\n",
       "      <th>BirthRate</th>\n",
       "      <th>TARGET_deathRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>404.300000</td>\n",
       "      <td>33975</td>\n",
       "      <td>8251</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.3</td>\n",
       "      <td>50.8</td>\n",
       "      <td>51.9</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>49.7</td>\n",
       "      <td>20.6</td>\n",
       "      <td>96.684036</td>\n",
       "      <td>0.438181</td>\n",
       "      <td>0.082899</td>\n",
       "      <td>0.272383</td>\n",
       "      <td>51.926207</td>\n",
       "      <td>5.041436</td>\n",
       "      <td>199.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>403.800000</td>\n",
       "      <td>47363</td>\n",
       "      <td>22702</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.8</td>\n",
       "      <td>39.8</td>\n",
       "      <td>42.7</td>\n",
       "      <td>...</td>\n",
       "      <td>46.8</td>\n",
       "      <td>31.6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>92.295459</td>\n",
       "      <td>2.102845</td>\n",
       "      <td>0.609648</td>\n",
       "      <td>0.879131</td>\n",
       "      <td>50.949545</td>\n",
       "      <td>6.329661</td>\n",
       "      <td>137.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>77222</td>\n",
       "      <td>9899</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.1</td>\n",
       "      <td>36.9</td>\n",
       "      <td>39.8</td>\n",
       "      <td>...</td>\n",
       "      <td>54.3</td>\n",
       "      <td>18.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>95.690422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523871</td>\n",
       "      <td>0.118612</td>\n",
       "      <td>64.532156</td>\n",
       "      <td>5.148130</td>\n",
       "      <td>126.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>429.600000</td>\n",
       "      <td>80650</td>\n",
       "      <td>48904</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.5</td>\n",
       "      <td>42.7</td>\n",
       "      <td>44.1</td>\n",
       "      <td>...</td>\n",
       "      <td>55.6</td>\n",
       "      <td>28.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>89.606996</td>\n",
       "      <td>7.407407</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>62.344481</td>\n",
       "      <td>5.627462</td>\n",
       "      <td>173.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>407.500000</td>\n",
       "      <td>42839</td>\n",
       "      <td>22255</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.1</td>\n",
       "      <td>30.2</td>\n",
       "      <td>31.6</td>\n",
       "      <td>...</td>\n",
       "      <td>46.5</td>\n",
       "      <td>26.8</td>\n",
       "      <td>18.1</td>\n",
       "      <td>79.587990</td>\n",
       "      <td>2.948701</td>\n",
       "      <td>8.482564</td>\n",
       "      <td>5.637090</td>\n",
       "      <td>63.005948</td>\n",
       "      <td>10.436469</td>\n",
       "      <td>179.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>335.000000</td>\n",
       "      <td>155</td>\n",
       "      <td>445.700000</td>\n",
       "      <td>41608</td>\n",
       "      <td>61109</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.728403</td>\n",
       "      <td>41.2</td>\n",
       "      <td>39.8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.4</td>\n",
       "      <td>42.6</td>\n",
       "      <td>23.4</td>\n",
       "      <td>95.398451</td>\n",
       "      <td>2.154399</td>\n",
       "      <td>0.448024</td>\n",
       "      <td>0.106749</td>\n",
       "      <td>49.736708</td>\n",
       "      <td>5.379260</td>\n",
       "      <td>201.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>113.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>497.300000</td>\n",
       "      <td>61259</td>\n",
       "      <td>17299</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.9</td>\n",
       "      <td>44.9</td>\n",
       "      <td>47.3</td>\n",
       "      <td>...</td>\n",
       "      <td>43.6</td>\n",
       "      <td>34.8</td>\n",
       "      <td>19.9</td>\n",
       "      <td>92.609104</td>\n",
       "      <td>3.320038</td>\n",
       "      <td>0.786016</td>\n",
       "      <td>0.997184</td>\n",
       "      <td>53.404362</td>\n",
       "      <td>3.227666</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>571.000000</td>\n",
       "      <td>210</td>\n",
       "      <td>457.200000</td>\n",
       "      <td>49790</td>\n",
       "      <td>118212</td>\n",
       "      <td>12.6</td>\n",
       "      <td>676.750245</td>\n",
       "      <td>35.4</td>\n",
       "      <td>34.5</td>\n",
       "      <td>36.3</td>\n",
       "      <td>...</td>\n",
       "      <td>56.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>13.7</td>\n",
       "      <td>91.484690</td>\n",
       "      <td>1.389174</td>\n",
       "      <td>4.463981</td>\n",
       "      <td>0.651015</td>\n",
       "      <td>47.707412</td>\n",
       "      <td>4.937288</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>1962.667684</td>\n",
       "      <td>7</td>\n",
       "      <td>453.549422</td>\n",
       "      <td>50886</td>\n",
       "      <td>2640</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.4</td>\n",
       "      <td>45.3</td>\n",
       "      <td>50.1</td>\n",
       "      <td>...</td>\n",
       "      <td>35.1</td>\n",
       "      <td>32.3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>96.892139</td>\n",
       "      <td>0.987203</td>\n",
       "      <td>0.548446</td>\n",
       "      <td>0.146252</td>\n",
       "      <td>62.436975</td>\n",
       "      <td>8.951965</td>\n",
       "      <td>136.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>741.000000</td>\n",
       "      <td>308</td>\n",
       "      <td>478.400000</td>\n",
       "      <td>45556</td>\n",
       "      <td>119980</td>\n",
       "      <td>13.8</td>\n",
       "      <td>300.050008</td>\n",
       "      <td>44.0</td>\n",
       "      <td>43.2</td>\n",
       "      <td>44.6</td>\n",
       "      <td>...</td>\n",
       "      <td>44.8</td>\n",
       "      <td>39.2</td>\n",
       "      <td>18.9</td>\n",
       "      <td>96.006176</td>\n",
       "      <td>0.790178</td>\n",
       "      <td>0.771187</td>\n",
       "      <td>0.406236</td>\n",
       "      <td>47.201757</td>\n",
       "      <td>4.823686</td>\n",
       "      <td>193.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2438 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avgAnnCount  avgDeathsPerYear  incidenceRate  medIncome  popEst2015  \\\n",
       "0       59.000000                30     404.300000      33975        8251   \n",
       "1      114.000000                41     403.800000      47363       22702   \n",
       "2       33.000000                11     352.000000      77222        9899   \n",
       "3      254.000000               100     429.600000      80650       48904   \n",
       "4       75.000000                32     407.500000      42839       22255   \n",
       "...           ...               ...            ...        ...         ...   \n",
       "2433   335.000000               155     445.700000      41608       61109   \n",
       "2434   113.000000                37     497.300000      61259       17299   \n",
       "2435   571.000000               210     457.200000      49790      118212   \n",
       "2436  1962.667684                 7     453.549422      50886        2640   \n",
       "2437   741.000000               308     478.400000      45556      119980   \n",
       "\n",
       "      povertyPercent  studyPerCap  MedianAge  MedianAgeMale  MedianAgeFemale  \\\n",
       "0               20.5     0.000000       51.3           50.8             51.9   \n",
       "1               13.8     0.000000       40.8           39.8             42.7   \n",
       "2                6.8     0.000000       38.1           36.9             39.8   \n",
       "3                7.5     0.000000       43.5           42.7             44.1   \n",
       "4               14.6     0.000000       31.1           30.2             31.6   \n",
       "...              ...          ...        ...            ...              ...   \n",
       "2433            17.5    32.728403       41.2           39.8             42.0   \n",
       "2434             9.0     0.000000       45.9           44.9             47.3   \n",
       "2435            12.6   676.750245       35.4           34.5             36.3   \n",
       "2436            10.4     0.000000       47.4           45.3             50.1   \n",
       "2437            13.8   300.050008       44.0           43.2             44.6   \n",
       "\n",
       "      ...  PctEmpPrivCoverage  PctPublicCoverage  PctPublicCoverageAlone  \\\n",
       "0     ...                26.0               49.7                    20.6   \n",
       "1     ...                46.8               31.6                    13.0   \n",
       "2     ...                54.3               18.2                     8.6   \n",
       "3     ...                55.6               28.8                    13.5   \n",
       "4     ...                46.5               26.8                    18.1   \n",
       "...   ...                 ...                ...                     ...   \n",
       "2433  ...                42.4               42.6                    23.4   \n",
       "2434  ...                43.6               34.8                    19.9   \n",
       "2435  ...                56.5               28.6                    13.7   \n",
       "2436  ...                35.1               32.3                    12.6   \n",
       "2437  ...                44.8               39.2                    18.9   \n",
       "\n",
       "       PctWhite  PctBlack  PctAsian  PctOtherRace  PctMarriedHouseholds  \\\n",
       "0     96.684036  0.438181  0.082899      0.272383             51.926207   \n",
       "1     92.295459  2.102845  0.609648      0.879131             50.949545   \n",
       "2     95.690422  0.000000  0.523871      0.118612             64.532156   \n",
       "3     89.606996  7.407407  0.870370      0.450617             62.344481   \n",
       "4     79.587990  2.948701  8.482564      5.637090             63.005948   \n",
       "...         ...       ...       ...           ...                   ...   \n",
       "2433  95.398451  2.154399  0.448024      0.106749             49.736708   \n",
       "2434  92.609104  3.320038  0.786016      0.997184             53.404362   \n",
       "2435  91.484690  1.389174  4.463981      0.651015             47.707412   \n",
       "2436  96.892139  0.987203  0.548446      0.146252             62.436975   \n",
       "2437  96.006176  0.790178  0.771187      0.406236             47.201757   \n",
       "\n",
       "      BirthRate  TARGET_deathRate  \n",
       "0      5.041436             199.5  \n",
       "1      6.329661             137.1  \n",
       "2      5.148130             126.9  \n",
       "3      5.627462             173.8  \n",
       "4     10.436469             179.8  \n",
       "...         ...               ...  \n",
       "2433   5.379260             201.5  \n",
       "2434   3.227666             160.0  \n",
       "2435   4.937288             160.0  \n",
       "2436   8.951965             136.2  \n",
       "2437   4.823686             193.2  \n",
       "\n",
       "[2438 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the training data and targets files\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "training_targets = pd.read_csv(training_targets_path)\n",
    "\n",
    "# Join the training data and targets into a single dataframe\n",
    "cancer = training_data.join(training_targets)\n",
    "\n",
    "# Print the dataframe\n",
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "cancer.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "cancer.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot histograms of all features to visualise their distributions and identify outliers. Do you notice any unusual values for any of the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of the features to visulaise their distributions and identify outliers\n",
    "cancer.hist(figsize=(20,20), bins=50, xlabelsize=8, ylabelsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Observations:\n",
    "- MedianAge has some extreme, potentially unrealistic values - in at least one county the median age exceeds 400 years which must be incorrect data\n",
    "- AvgHouseholdSize has a number of zero or near zero values which implies that most homes in a county are unoccupied\n",
    "- Some counties have 0% Black, Asian and Other Races which is unlikely given the population of the counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correlations of all features with the target variable (across the data set) and sort them according the strength of correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlations of all features with the target variable (i.e. the correlation matrix)\n",
    "correlations = cancer.corr().abs()\n",
    "correlations_TARGET = correlations['TARGET_deathRate'].sort_values(ascending=False)\n",
    "correlations_TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which are the top five features with strongest correlations to the targets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top five features with the highest correlation with the target variable\n",
    "top_five_features = correlations_TARGET[1:6]\n",
    "print(top_five_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Five Features are:\n",
    "1) PctBachDeg25_Over\n",
    "2) incidenceRate\n",
    "3) PctPublicCoverageAlone\n",
    "4) medIncome\n",
    "5) povertyPercent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot these correlations using the scatter matrix plotting function available in pandas and comment on at least two sets of features that show visible correlations to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top five features correlations with each other using the scatter matrix plotting function\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(cancer[top_five_features.index], figsize=(12, 8))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Observations:\n",
    "- PctPublicCoverageAlone and poveryPercent have an obvious positive correlation - makes sense given private coverage is expensive\n",
    "- medIncome and PctBachDeg25_Over have a positive correlation as well - degrees are expensive\n",
    "- medIncome and PctPublicCoverageAlone have a negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wSA10GzM_Xu"
   },
   "source": [
    "# **Exercise 2**\n",
    "\n",
    "Create an ML pipeline using scikit-learn (as demonstrated in the lab notebooks) to pre-process the training data. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0JJ21KooNRVT"
   },
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with too many missing values\n",
    "class RemoveFeatures(BaseEstimator):\n",
    "    def __init__(self, threshold=0.1):\n",
    "        self.threshold = threshold\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # Compute the number of missing values in each feature\n",
    "        missing_values = X.isnull().sum()\n",
    "        # Compute the proportion of missing values in each feature\n",
    "        missing_values_prop = missing_values / len(X)\n",
    "        # Remove features with a proportion of missing values greater than the threshold\n",
    "        X = X.drop(missing_values_prop[missing_values_prop > self.threshold].index, axis=1)\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from existing features in the cancer dataset to improve the predictive power of the model\n",
    "class NewFeatures(BaseEstimator):\n",
    "    def __init__(self, columns_ix=None):\n",
    "        self.avgAnnCount_ix = columns_ix[0]\n",
    "        self.avgDeathsPerYear_ix = columns_ix[1]\n",
    "        self.popEst2015_ix = columns_ix[2]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.avgAnnCount = X['avgAnnCount'].values\n",
    "        self.avgDeathsPerYear = X['avgDeathsPerYear'].values\n",
    "        self.popEst2015 = X['popEst2015'].values\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X['CancerDiagnosisPerCapita'] = self.avgAnnCount / self.popEst2015\n",
    "        X['CancerDeathsPerDiagnosis'] = self.avgDeathsPerYear / self.avgAnnCount\n",
    "        X['CancerDeathsPerCapita'] = self.avgDeathsPerYear / self.popEst2015\n",
    "        return np.c_[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to preprocess the data\n",
    "pipeline = Pipeline([\n",
    "    ('remove_features', RemoveFeatures(threshold=0.1)),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the cancer dataset into features and targets\n",
    "X = cancer.drop('TARGET_deathRate', axis=1)\n",
    "X_names = X.columns\n",
    "y = cancer['TARGET_deathRate'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the features\n",
    "X_preprocessed = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiM5kym6NRww"
   },
   "source": [
    "# **Exercise 3**\n",
    "\n",
    "Fit linear regression models to the pre-processed data using: Ordinary least squares (OLS), Lasso and Ridge models. Choose suitable regularisation weights for Lasso and Ridge regression and include a description in text of how they were chosen. In your submitted solution make sure you set the values for the regularisation weights equal to those you identify from your experiment(s). Quantitatively compare your results from all three models and report the best performing one. Include code for all steps above. (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bv9wFbldPo45"
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear (ols) regression\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the test set\n",
    "y_pred = ols.predict(X_test)\n",
    "\n",
    "# Get sample of the predictions\n",
    "ols_comp = pd.DataFrame({\n",
    "    'Actual': y_test, \n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "print(\"Predicted vs. Actual for Linear Regression\", \"\\n\", ols_comp.sample(10, random_state=random_seed))\n",
    "\n",
    "# Print the root mean squared error\n",
    "print(\"\\nRoot Mean Squared Error: %.5f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Analysis\n",
    "Use a range of alphas and examine with respect to the RMS Error of the Lasso Regression Model predictions to get a range\n",
    "in which further analysis of alpha values can be made to get an optimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best alpha value for the lasso regression model\n",
    "lasso = linear_model.Lasso(random_state=random_seed)\n",
    "\n",
    "errors = []\n",
    "alphas = np.logspace(-4, 4, 200)\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    y_pred = lasso.predict(X_test)\n",
    "    errors.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, errors)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rms error\")\n",
    "plt.axis(\"tight\")\n",
    "plt.title(\"RMS Error as a function of regularisation: Lasso Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Use Grid Search Cross Validation with 5 cross validations and negative mean squared error scoring to evaluate 51 values of alpha between\n",
    "1e-3 and 1e+3 and from that, get the best alpha value that minimises error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search to find the best alpha value for the lasso regression model\n",
    "lasso = linear_model.Lasso(random_state=random_seed)\n",
    "alphas = np.logspace(-3, 3, 51)\n",
    "param_grid = {'alpha': list(alphas)}\n",
    "\n",
    "lgrid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "lgrid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha value: \", lgrid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "Use Random Search Cross Validation with 5 cross validations to find the best performing alpha value in the range 1e-3 and\n",
    "1e+3, similar to Grid Search but randomly samples alpha values in a range to test rather than systematically and exhaustively\n",
    "searching all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Random Search to find the best alpha value for the lasso regression model\n",
    "lasso = linear_model.Lasso(random_state=random_seed)\n",
    "param_grid = {'alpha': loguniform(1e-3, 1e3)}\n",
    "n_iter_search = 30\n",
    "\n",
    "lrand_search = RandomizedSearchCV(lasso, param_grid, n_iter=n_iter_search, cv=5, \n",
    "    scoring='neg_mean_squared_error', return_train_score=True, random_state=random_seed)\n",
    "lrand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha value: \", lrand_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Analysis\n",
    "Use a range of alphas and examine with respect to the RMS Error of the Ridge Regression Model predictions to get a range\n",
    "in which further analysis of alpha values can be made to get an optimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best alpha value for the ridge regression model\n",
    "ridge = linear_model.Ridge(random_state=random_seed)\n",
    "\n",
    "errors = []\n",
    "alphas = np.logspace(-10, 10, 200)\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha=a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_pred = ridge.predict(X_test)\n",
    "    errors.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, errors)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rms error\")\n",
    "plt.axis(\"tight\")\n",
    "plt.title(\"RMS Error as a function of regularisation: Ridge Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "Use Grid Search Cross Validation with 5 cross validations and negative mean squared error scoring to evaluate 51 values of alpha between\n",
    "1e-1 and 1e+6 and from that, get the best alpha value that minimises error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search to find the best alpha value for the ridge regression model\n",
    "ridge = linear_model.Ridge(random_state=random_seed)\n",
    "alphas = np.logspace(-1, 6, 51)\n",
    "param_grid = {'alpha': list(alphas)}\n",
    "\n",
    "rgrid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "rgrid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha value: \", rgrid_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "Use Random Search Cross Validation with 5 cross validations to find the best performing alpha value in the range 1e-1 and\n",
    "1e+6, similar to Grid Search but randomly samples alpha values in a range to test rather than systematically and exhaustively\n",
    "searching all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Random Search to find the best alpha value for the ridge regression model\n",
    "ridge = linear_model.Ridge(random_state=random_seed)\n",
    "param_grid = {'alpha': loguniform(1e-1, 1e6)}\n",
    "n_iter_search = 30\n",
    "\n",
    "rrand_search = RandomizedSearchCV(ridge, param_grid, n_iter=n_iter_search, cv=5,\n",
    "    scoring='neg_mean_squared_error', return_train_score=True, random_state=random_seed)\n",
    "rrand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha value: \", rrand_search.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Values:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regressor: \n",
    "- Use Alpha = 0.06\n",
    "- Both RandomSearch and GridSearch gave values near 0.06 for alpha and looking at the visualisation, high accuracy with respect\n",
    "to decimal points is not needed to such a degree (within orders of magintude seems to be sufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression with the best alpha value @ alpha=0.06\n",
    "lasso = linear_model.Lasso(alpha=0.06, random_state=random_seed)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the test set\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Get sample of the predictions\n",
    "lasso_comp = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "print(\"Predicted vs. Actual for Lasso Regression\", \"\\n\", lasso_comp.sample(10, random_state=random_seed))\n",
    "\n",
    "# Print the root mean squared error\n",
    "print(\"\\nRoot Mean Squared Error: %.5f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regressor:\n",
    "- Use Alpha = 30\n",
    "- Both RandomSearch and GridSearch gave optimised alpha values around 30, looking at the graph any alpha within the same order of magnitude seems to give similar errors so 30 was decided as the alpha value for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with the best alpha value @ alpha=30\n",
    "ridge = linear_model.Ridge(alpha=30, random_state=random_seed)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the test set\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "# Get sample of the predictions\n",
    "ridge_comp = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "print(\"Predicted vs. Actual for Ridge Regression\", \"\\n\", ridge_comp.sample(10, random_state=random_seed))\n",
    "\n",
    "# Print the root mean squared error\n",
    "print(\"\\nRoot Mean Squared Error: %.5f\" % np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantative Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the cross validation scores\n",
    "def display_scores(scores, method=\"\"):\n",
    "    print(method, \"Scores:\", scores)\n",
    "    print(method, \"Scores Mean:\", scores.mean())\n",
    "    print(method, \"Scores Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation on the linear regression model\n",
    "# Get root mean squared error\n",
    "ols_scores = cross_val_score(ols, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "ols_rmse_scores = np.sqrt(-ols_scores)\n",
    "# Get R-squared value\n",
    "ols_r2_scores = cross_val_score(ols, X_train, y_train, scoring=\"r2\", cv=10)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "display_scores(ols_rmse_scores, method=\"RMSE\")\n",
    "display_scores(ols_r2_scores, method=\"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation on the lasso regression model\n",
    "# Get root mean squared error\n",
    "lasso_scores = cross_val_score(lasso, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lasso_rmse_scores = np.sqrt(-lasso_scores)\n",
    "# Get R-squared value\n",
    "lasso_r2_scores = cross_val_score(lasso, X_train, y_train, scoring=\"r2\", cv=10)\n",
    "\n",
    "print(\"Lasso Regression:\")\n",
    "display_scores(lasso_rmse_scores, method=\"RMSE\")\n",
    "display_scores(lasso_r2_scores, method=\"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation on the ridge regression model\n",
    "# Get root mean squared error\n",
    "ridge_scores = cross_val_score(ridge, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "ridge_rmse_scores = np.sqrt(-ridge_scores)\n",
    "# Get R-squared value\n",
    "ridge_r2_scores = cross_val_score(ridge, X_train, y_train, scoring=\"r2\", cv=10)\n",
    "\n",
    "print(\"Ridge Regression:\")\n",
    "display_scores(ridge_rmse_scores, method=\"RMSE\")\n",
    "display_scores(ridge_r2_scores, method=\"R2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "- Looking at the cross validation of the ols, lasso and ridge regression models they seem to be performing similarily with\n",
    "only marginal differences between both RMSE and R-squared values\n",
    "\n",
    "<center>\n",
    "\n",
    "| Regression Model | Evaluation Method | Scores Mean | Scores Sandard Deviation |\n",
    "| ---------------- | ----------------- | ----------- | ------------------------ |\n",
    "| OLS | RMSE | 19.561 | 1.2003 |\n",
    "| Lasso | RMSE | 19.525 | 1.2031 |\n",
    "| Ridge | RMSE | 19.524 | 1.1980 |\n",
    "| OLS | R2 | 0.4723 | 0.0790 |\n",
    "| Lasso | R2 | 0.4746 | 0.0761 |\n",
    "| Ridge | R2 | 0.4747 | 0.0758 |\n",
    "\n",
    "</center>\n",
    "\n",
    "- Ridge has the lowest RMS Error and Standard Deviation meaning it performs most consistently with minimal error compared to the other models\n",
    "- Ridge also has the greatest R-Squared Score and Standard Deviation meaning meaning that it fits the data the most compared to the other models in may therefore be better though the value of around 0.47 is not the best r-squared score - there is at least 50% of the data the model isn't fit to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdx4FmjqPpVJ"
   },
   "source": [
    "# **Exercise 4**\n",
    "\n",
    "Use Lasso regression and the best regularisation weight identified from Exercise 3 to identify the five most important/relevant features for the provided data set and regression task. Report what these are desceding order of their importance. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7vC63VPnQXI_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: incidenceRate, Importance: 10.744938577691379\n",
      "Feature: PctHS25_Over, Importance: 7.125970680290926\n",
      "Feature: PctAsian, Importance: 6.7131409981870585\n",
      "Feature: PctUnemployed16_Over, Importance: 6.164424420629066\n",
      "Feature: PercentMarried, Importance: 6.09862455197102\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regularisation with the best alpha value @ alpha=0.06\n",
    "lasso = linear_model.Lasso(alpha=0.06)\n",
    "lasso.fit(X_preprocessed, y)\n",
    "coefficients = lasso.coef_\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "feature_importance = zip(X_names, importance)\n",
    "top_five_features = sorted(feature_importance, key=lambda x: -x[1])[:5]\n",
    "for feature, importance in top_five_features:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQvZqdqGQ_pH"
   },
   "source": [
    "# **Exercise 5**\n",
    "\n",
    "Fit a Random Forest regression model to the training data and quantitatively evaluate and compare the Random Forest regression model with the best linear regression model identified from Exercise 3. Report which model provides the best results. Next, report the top five most important/relevant features for the provided data set and regression task identified using the Random Forest model. Comment on how these compare with the features identified from Lasso regression? (14 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbFXu6UiQ-gv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg-Tksx5QXiL"
   },
   "source": [
    "# **Exercise 6**\n",
    "\n",
    "Use the provided test example data ('Test_data_example.csv' and 'Test_data_example_targets.csv') to write an inference script to evaluate the best regression model identified from preceding exercises. First re-train the chosen regression model using all of the provided training data and test your predictions on the provided example test data. Note - the final evaluation of your submission will be done by replacing this example test data with held out (unseen) test data that is not provided to you. But the format of this \"unseen\" test data will be identical to the example test data provided to you. Use the code snippet provided below to prepare your inference script to predict targets for the unseen test data. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcmuUJMODD2Q"
   },
   "outputs": [],
   "source": [
    "## Read in the provided example test data\n",
    "test_data_path = 'Test_data_example.csv'\n",
    "test_targets_path ='Test_data_example_targets.csv'\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_targets = pd.read_csv(test_targets_path)\n",
    "## Retrain your chosen regression model here \n",
    "# For example: lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X_train,y_train) where X_train and y_train is provided training data\n",
    "# Next write the lines of code required to predict on unseen test data and evaluate your predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5611M-Coursework Assessment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('jupyterenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b1b34da9613a0d748d26328173513920323e58465e7e994b8b5c165cac08fbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
